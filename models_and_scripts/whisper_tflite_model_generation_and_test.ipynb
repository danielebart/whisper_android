{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vilassn/whisper_android/blob/master/models_and_scripts/whisper_tflite_model_generation_and_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AveoTQ563b3E"
      },
      "source": [
        "##Install TensorFlow, Tranformers and datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3Dhj15CQ3ath"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow==2.14.0\n",
        "!pip install transformers\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b2AJbu5CJ9Z"
      },
      "source": [
        "## Configure model to be generated as per requirement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUj7pZUWCJUI"
      },
      "outputs": [],
      "source": [
        "# Set model (whisper-tiny, whisper-tiny.en, whisper-base, whisper-base.en, whisper-small, whisper-small.en)\n",
        "model_name = \"whisper-base\"\n",
        "\n",
        "# Store the mappings in a dictionary (For codes, refer: https://huggingface.co/openai/whisper-large/blob/main/added_tokens.json)\n",
        "code_mappings = {\n",
        "    \"<|en|>\": 50259,\n",
        "    \"<|fr|>\": 50265,\n",
        "    \"<|hi|>\": 50276,\n",
        "    \"<|ko|>\": 50264,\n",
        "    \"<|nocaptions|>\": 50362,\n",
        "    \"<|notimestamps|>\": 50363,\n",
        "    \"<|transcribe|>\": 50359,\n",
        "    \"<|translate|>\": 50358,\n",
        "}\n",
        "\n",
        "# Define the language, task, and options\n",
        "language_code = \"<|en|>\"\n",
        "task_code = \"<|transcribe|>\"\n",
        "option_code = \"<|notimestamps|>\"\n",
        "\n",
        "# Construct forced_decoder_ids using the mappings\n",
        "forced_decoder_ids = [\n",
        "    [1, code_mappings[language_code]],\n",
        "    [2, code_mappings[task_code]],\n",
        "    [3, code_mappings[option_code]]\n",
        "]\n",
        "\n",
        "print(forced_decoder_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ng4Qq2rejVOE"
      },
      "source": [
        "##Import the libraries, load the model, do the inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "HKfmQ2kp627t"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import transformers\n",
        "import datasets\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import WhisperProcessor, WhisperFeatureExtractor, TFWhisperForConditionalGeneration, WhisperTokenizer\n",
        "\n",
        "pretrained_model = f\"openai/{model_name}\"\n",
        "tflite_model_path = f\"{model_name}.tflite\"\n",
        "saved_model_dir = f\"tf_{model_name}_saved\"\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(pretrained_model)\n",
        "tokenizer = WhisperTokenizer.from_pretrained(pretrained_model, predict_timestamps=True)\n",
        "processor = WhisperProcessor(feature_extractor, tokenizer)\n",
        "model = TFWhisperForConditionalGeneration.from_pretrained(pretrained_model)\n",
        "\n",
        "# Loading dataset\n",
        "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
        "inputs = feature_extractor(ds[0][\"audio\"][\"array\"], sampling_rate=ds[0][\"audio\"][\"sampling_rate\"], return_tensors=\"tf\")\n",
        "input_features = inputs.input_features\n",
        "\n",
        "# Generating Transcription\n",
        "generated_ids = model.generate(input_features=input_features)\n",
        "print(generated_ids)\n",
        "\n",
        "transcription = processor.tokenizer.decode(generated_ids[0])\n",
        "print(transcription)\n",
        "\n",
        "# Save the model\n",
        "# model.save(saved_model_dir) # not need to save here, saving using tf.saved_model.save() call"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EA65NFFIj4bN"
      },
      "source": [
        "## Prompt fix, patch to make forced_decoder_ids work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1VOFBO9_MO-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from transformers import TFForceTokensLogitsProcessor, TFLogitsProcessor\n",
        "from typing import List, Optional, Union, Any\n",
        "\n",
        "# Patching methods of class TFForceTokensLogitsProcessor(TFLogitsProcessor):\n",
        "\n",
        "def my__init__(self, force_token_map: List[List[int]]):\n",
        "    force_token_map = dict(force_token_map)\n",
        "    # Converts the dictionary of format {index: token} containing the tokens to be forced to an array, where the\n",
        "    # index of the array corresponds to the index of the token to be forced, for XLA compatibility.\n",
        "    # Indexes without forced tokens will have an negative value.\n",
        "    force_token_array = np.ones((max(force_token_map.keys()) + 1), dtype=np.int32) * -1\n",
        "    for index, token in force_token_map.items():\n",
        "        if token is not None:\n",
        "            force_token_array[index] = token\n",
        "    self.force_token_array = tf.convert_to_tensor(force_token_array, dtype=tf.int32)\n",
        "\n",
        "def my__call__(self, input_ids: tf.Tensor, scores: tf.Tensor, cur_len: int) -> tf.Tensor:\n",
        "    def _force_token(generation_idx):\n",
        "        batch_size = scores.shape[0]\n",
        "        current_token = self.force_token_array[generation_idx]\n",
        "\n",
        "        # Original code below generates NaN values when the model is exported to tflite\n",
        "        # it just needs to be a negative number so that the forced token's value of 0 is the largest\n",
        "        # so it will get chosen\n",
        "        #new_scores = tf.ones_like(scores, dtype=scores.dtype) * -float(\"inf\")\n",
        "        new_scores = tf.ones_like(scores, dtype=scores.dtype) * -float(1)\n",
        "        indices = tf.stack((tf.range(batch_size), tf.tile([current_token], [batch_size])), axis=1)\n",
        "        updates = tf.zeros((batch_size,), dtype=scores.dtype)\n",
        "        new_scores = tf.tensor_scatter_nd_update(new_scores, indices, updates)\n",
        "        return new_scores\n",
        "\n",
        "    scores = tf.cond(\n",
        "        tf.greater_equal(cur_len, tf.shape(self.force_token_array)[0]),\n",
        "        # If the current length is geq than the length of force_token_array, the processor does nothing.\n",
        "        lambda: tf.identity(scores),\n",
        "        # Otherwise, it may force a certain token.\n",
        "        lambda: tf.cond(\n",
        "            tf.greater_equal(self.force_token_array[cur_len], 0),\n",
        "            # Only valid (positive) tokens are forced\n",
        "            lambda: _force_token(cur_len),\n",
        "            # Otherwise, the processor does nothing.\n",
        "            lambda: scores,\n",
        "        ),\n",
        "    )\n",
        "    return scores\n",
        "\n",
        "TFForceTokensLogitsProcessor.__init__ = my__init__\n",
        "TFForceTokensLogitsProcessor.__call__ = my__call__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5c0ZSGWj9Gq"
      },
      "source": [
        "##Define a model with a serving signature and save it in TF SavedModel format.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K968u9QS5jPL"
      },
      "outputs": [],
      "source": [
        "class GenerateModel(tf.Module):\n",
        "  def __init__(self, model):\n",
        "    super(GenerateModel, self).__init__()\n",
        "    self.model = model\n",
        "\n",
        "  @tf.function(\n",
        "    # shouldn't need static batch size, but throws exception without it (needs to be fixed)\n",
        "    input_signature=[\n",
        "      tf.TensorSpec((1, 80, 3000), tf.float32, name=\"input_features\"),\n",
        "    ],\n",
        "  )\n",
        "  def serving(self, input_features):\n",
        "    outputs = self.model.generate(\n",
        "      input_features,\n",
        "      # change below if you think your output will be bigger\n",
        "      # aka if you have bigger transcriptions\n",
        "      # you can make it 200 for example\n",
        "      max_new_tokens=448,\n",
        "      return_dict_in_generate=True,\n",
        "      forced_decoder_ids=forced_decoder_ids,\n",
        "    )\n",
        "    return {\"sequences\": outputs[\"sequences\"]}\n",
        "\n",
        "generate_model = GenerateModel(model=model)\n",
        "tf.saved_model.save(generate_model, saved_model_dir, signatures={\"serving_default\": generate_model.serving})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLfY_Fv4qHSC"
      },
      "source": [
        "## Convert the model from TF SavedModel format to TF lite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBD7wMdQnsYc"
      },
      "outputs": [],
      "source": [
        "# Convert the model\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
        "converter.target_spec.supported_ops = [\n",
        "  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
        "  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n",
        "]\n",
        "\n",
        "# Learn about post training quantization\n",
        "# https://www.tensorflow.org/lite/performance/post_training_quantization\n",
        "\n",
        "# Dynamic range quantization which reduces the size of the model to 25%\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "# Float16 quantization reduces the size to 50%\n",
        "# converter.target_spec.supported_types = [tf.float16]\n",
        "\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model\n",
        "with open(tflite_model_path, 'wb') as f:\n",
        "    f.write(tflite_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jRRP3nQkEtC"
      },
      "source": [
        "##Test tflite model using TFLite Interpreter. Check transcription for dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3fbeAWE5ikD"
      },
      "outputs": [],
      "source": [
        "# loaded model... now with generate!\n",
        "interpreter = tf.lite.Interpreter(tflite_model_path)\n",
        "\n",
        "tflite_generate = interpreter.get_signature_runner()\n",
        "generated_ids = tflite_generate(input_features=input_features)[\"sequences\"]\n",
        "# print(generated_ids)\n",
        "\n",
        "transcription = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
        "print(transcription)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOiytfUd4yyp"
      },
      "source": [
        "## Install faster-whisper for audio processing and testing model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-k7BGLqM5Egh"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/SYSTRAN/faster-whisper.git\n",
        "!pip install faster-whisper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO1ZerhS4-in"
      },
      "source": [
        "## Test all audio files in loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Oc3YEK2j1FLb",
        "outputId": "8f8348ab-b394-449a-c9ad-57b3ddddfdd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1.......................................................\n",
            "\n",
            " पुदन्वारा तुद्वारा तुद्वारा तुद्वारा तुद्वारा तुद्वारा तुद्वारा तुद्वारा तुद्वारा तुद्वारा तुद्वारा तुद्वारा तुद्वारा तुद्वारा तुद्वारा तुद्वारा तुद्वारा तुद्वारा तुद्वारा तुद्वारा तुद्वारा तुद्वारा तुद्वारा तुद्वारा तुद्वारा तुद्वारा तुद्वारा तुद्वारा तुद्वारा तुद्वारा तुद्वारा तुद्वारा तुद्वारा तुद्वारा तुद्वारा तुद्वारा तुद्वारा\n",
            "\n",
            " La marcha tuvo su punto de reunión en la plaza de los ministerios para terminar luego en la plaza de mayo en la convocatoria. Nosotros como parte de Revolte Studentil, nos unimos con los chicos del más, en Villamaria, que es el centro estudiante de sociales allá en las altas gays de estudios, junto con los chicos de la PAPA Colectiva, que son de humanas y\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from transformers import WhisperProcessor, WhisperFeatureExtractor\n",
        "from faster_whisper import decode_audio\n",
        "\n",
        "# Set up paths and model (whisper-tiny, whisper-tiny.en, whisper-base, whisper-base.en, whisper-small, whisper-small.en)\n",
        "# model_name = \"whisper-base.en\"\n",
        "# pretrained_model = f\"openai/{model_name}\"\n",
        "# tflite_model_path = f\"{model_name}.tflite\"\n",
        "\n",
        "############ NOTE: Specify the folder containing audio files\n",
        "audio_folder_path = '/content/drive/MyDrive/Colab Notebooks/audio'\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(pretrained_model)\n",
        "tokenizer = WhisperTokenizer.from_pretrained(pretrained_model, predict_timestamps=True)\n",
        "processor = WhisperProcessor(feature_extractor, tokenizer)\n",
        "\n",
        "interpreter = tf.lite.Interpreter(tflite_model_path)\n",
        "tflite_generate = interpreter.get_signature_runner()\n",
        "\n",
        "# Number of iterations you want the loop to run\n",
        "iterations = 1000\n",
        "\n",
        "for i in range(1, iterations + 1):  # Start from 1 to print iteration number\n",
        "    print(f\"Iteration {i}.......................................................\\n\")  # Print iteration number and newline\n",
        "\n",
        "    # Loop through all files in the folder\n",
        "    for audio_file_name in os.listdir(audio_folder_path):\n",
        "        audio_file_path = os.path.join(audio_folder_path, audio_file_name)\n",
        "\n",
        "        if audio_file_name.endswith('.wav'):  # Process only .wav files\n",
        "            # print(f\"Processing {audio_file_name}...\")\n",
        "\n",
        "            # Preprocess the audio file\n",
        "            input_audio = decode_audio(audio_file_path, sampling_rate=16000)\n",
        "            input_features = feature_extractor(input_audio, sampling_rate=16000, return_tensors=\"tf\").input_features\n",
        "\n",
        "            # Run the model\n",
        "            generated_ids = tflite_generate(input_features=input_features)[\"sequences\"]\n",
        "\n",
        "            # Decode and print transcription\n",
        "            transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "            print(f\"{transcription}\\n\")  # Add newline after each transcription"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/vilassn/whisper_android/blob/master/models_and_scripts/whisper_tflite_model_generation_and_test.ipynb",
      "authorship_tag": "ABX9TyPnyP38A9dXFqu8zfd1jRSJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}